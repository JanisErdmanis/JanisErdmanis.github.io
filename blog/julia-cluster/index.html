<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/highlight/styles/github.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/basic.css"> <link rel=icon  href="/assets/favicon.png"> <title>Julia on a PBS cluster</title> <header> <div class=blog-name ><a href="/">Janis Erdmanis</a></div> <nav> <ul> <li><a href="/cv/">CV</a> <li><a href="/blog/">Blog</a> <li><a href="/bookshelf/">BookShelf</a> </ul> </nav> </header> <div class=post-meta >Feb 25, 2019 | 2320 Words</div> <div class=franklin-content > <h1 id=julia_on_a_pbs_cluster ><a href="#julia_on_a_pbs_cluster" class=header-anchor >Julia on a PBS cluster</a></h1> <p><strong>Disclaimer: some code and commands had not been tested.</strong></p> <p>A time once comes when computation is too expensive to run. If you are lucky, you can adjust the parameters of the numerical method or find a better one. If that is not the case, you are now looking at what the phrase parallel computing promises.</p> <p>Often the numerical tasks can be reduced to actor-model, meaning that there is a single master giving orders to the computing slaves. Most programming languages give many tools for these capabilities; however, the way and complexity can alienate us from using them. This is another strength of Julia which shows us a better and more elegant way, which is this blog post.</p> <h2 id=minimal_set_of_julia_tools_for_parallel_computing ><a href="#minimal_set_of_julia_tools_for_parallel_computing" class=header-anchor >Minimal set of Julia tools for parallel computing</a></h2> <p>The most straightforward problem to parallelise is one where you know the domain in advance on which you attempt to calculate the function values or its aggregation, like integral or stochastic average. Another set of problems parallel computing can tackle is when the Domain elements are generated by some function learnt from the previous evaluation history. For that, I have written a small package <code>TaskMaster.jl,</code> inspired by my colleague&#39;s work on the Python adaptive package, which I plan to discuss in another post.</p> <pre><code class="julia hljs">Domain -&gt; f -&gt; Values
Domain -&gt; f -&gt; Sum(Values)</code></pre> <p>Before one can use the parallel computing abilities of the code, we need to discuss how to make the code parallel with the abstractions offered. Fortunately, there is no difference in the code whether a cluster or a local machine runs it. On the latter, we can start Julia in a command line with two worker processes as simple as <code>julia -p 2</code>.</p> <p>With such a command, REPL &#40;an interactive compiler shell&#41; would open on the master process, so, in the end, we end up with 3 Julia processes. From the REPL, we can run the function evaluation:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> Distributed <span class=hljs-comment ># The package containing abstractions</span>
<span class=hljs-meta >@everywhere</span> f(x) = x^<span class=hljs-number >2</span> <span class=hljs-comment >#defines function on all slaves</span>
res = pmap(f, <span class=hljs-number >1</span>:<span class=hljs-number >10</span>) <span class=hljs-comment >#runs the function splitting the work evenly between workers</span></code></pre> <p>As one can see, parallelism took us only three lines of code. To see that the code has been executed on all cores &#40;if you are suspicious ;&#41;, you can add a print statement in the function and see that the work has been split.</p> <p>In most cases, <code>pmap</code> is enough. However, sometimes we need more. Let&#39;s say we would like to evaluate a numerical derivative of one expensive function at a few points, and let&#39;s say the number of points is more minor than the available cores we can get.</p> <p>On the first try, one would do the following:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> Distributed
<span class=hljs-meta >@everywhere</span> f(x) = x^<span class=hljs-number >2</span>
h = <span class=hljs-number >0.1</span>
y0 = pmap(f, <span class=hljs-number >1</span>:<span class=hljs-number >10</span>)
y1 = pmap(f, <span class=hljs-number >1</span>:<span class=hljs-number >10</span> + h)
derivative = (y1-y0)./h</code></pre> <p>However, in this particular situation, that would mean that some workers would be idle. To overcome that, we can start a map asynchronously which works well with parallel processing.</p> <p>The <code>@async</code> macro makes the line run parallel to a master process with all other commands. Whereas <code>@sync</code> macro determines which processes we must wait to finish before proceeding further. Thus to resolve the shortcoming of the previous code, we could modify it as follows:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> Distributed
<span class=hljs-meta >@everywhere</span> f(x) = x^<span class=hljs-number >2</span>
h = <span class=hljs-number >0.1</span>
<span class=hljs-meta >@sync</span> <span class=hljs-keyword >begin</span>
    <span class=hljs-meta >@async</span> y0 = pmap(f, <span class=hljs-number >1</span>:<span class=hljs-number >10</span>)
    <span class=hljs-meta >@async</span> y1 = pmap (f, <span class=hljs-number >1</span>:<span class=hljs-number >10</span> + h)
<span class=hljs-keyword >end</span>
derivative = (y1-y0)./h</code></pre> <p>A surprising aspect of the code above is that both <code>pmap</code> functions would not try to chase the same workers, leaving some idle. That is achieved by keeping track of the previously used worker with a function <code>Distributed.nextproc&#40;&#41;</code>.</p> <p>So far, we have yet to consider how we can allocate a task to a particular worker. For that command, <code>@sapwnat</code> is designed &#40;there is also <code>@spawn</code> macro which uses worker given by <code>Distributed.nextproc&#40;&#41;</code>. To execute a function call on the second worker, one does the following:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> Distributed
<span class=hljs-meta >@everywhere</span> f(x) = x^<span class=hljs-number >2</span>
ref = <span class=hljs-meta >@spawnat</span> <span class=hljs-number >2</span> f(<span class=hljs-number >2</span>) <span class=hljs-comment ># returns a reference</span>
isready(ref) <span class=hljs-comment ># would return true if the task finishes.</span>
result = fetch(ref)</code></pre> <p>The <code>@spawnat</code> returns a reference which allows to kill the task, check if it is already finished and fetch the result from the worker.</p> <p>That is the minimum one needs to know to feel comfortable running calculations on multiple cores or even implementing your <code>pmap</code> function implementation &#40;I encourage looking at an example in Julia&#39;s manual&#41;. The next step is to learn how to use parallelised code on the cluster.</p> <h2 id=the_cluster ><a href="#the_cluster" class=header-anchor >The cluster</a></h2> <p>Now you know how to name your programs parallel, but at some point, local computer resources are needed. In such cases, you might look on of using. The cluster usually is a farm of servers joined together with fast network connections. If ssh access is possible to each node, it is relatively easy to start using it.</p> <p>For example, assuming that Julia had been set up, then for two PCs with ssh access, &quot;user@A&quot; and &quot;user@B&quot; can be used to initiate the cluster:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> Distributed
addprocs(<span class=hljs-string >&quot;user@A&quot;</span>, <span class=hljs-number >2</span>) <span class=hljs-comment ># creates two workers on machine A</span>
addprocs(<span class=hljs-string >&quot;user@B&quot;</span>, <span class=hljs-number >3</span>) <span class=hljs-comment ># creates three workers on machine B</span>
<span class=hljs-meta >@everywhere</span> f(x) = run(<span class=hljs-string >`hostname`</span>)
pmap(f, <span class=hljs-number >1</span>:<span class=hljs-number >10</span>)</code></pre> <p>Voila, that was all that we needed&#33; Since such initialisation is relatively common, Julia offers a command line option for that <code>julia --machinefile MachineFile</code>, where <code>MachineFile</code> contains the list of hosts. That is reasonably convenient to initialise Julia in the Cluster environment.</p> <h2 id=scheduling ><a href="#scheduling" class=header-anchor >Scheduling</a></h2> <p>The one thing which every cluster is going to have is a job scheduler - a program which stands in your way before you can ssh to the nodes &#40;unless you own the cluster&#41;. A job scheduler delegates resources according to the cluster&#39;s owner policies. Those define how many computing resources you can use and how long you need to wait for them. Although it is a simple task, there is no universal standard for scheduling jobs; to list a few, we have PBS, Torque, SLURM, etc. Fortunately, the basics should apply to any cluster.</p> <p>I am considering the PBS scheduling system. First, let&#39;s try to run a useful test script to see how stuff works:</p> <pre><code class="bash hljs">!<span class=hljs-comment >#/usr/bin/bash</span>
<span class=hljs-built_in >cat</span> <span class=hljs-variable >$MachinFile</span> &gt;&gt; test.log</code></pre> <p>To make it executable, we first give permission for this file to be executable by <code>chmod &#43;x ./test</code> &#40;assuming the <code>test</code> is the filename&#41;. At that point, we can schedule this script to be run on the cluster:</p> <pre><code class="bash hljs">qsub -l nodes=2:ppn=2 ./test</code></pre>
<p>That tells the scheduler to run the job on two different nodes with two cores on each of them. To see the job status, we can run <code>qstat</code> in the terminal. That also allows us to see how occupied the cluster is with the jobs.</p>
<p>The second argument in a <code>qsub</code> is a resource list. There one can say what kind of nodes one wants. For example, one can specify nodes with the newest processors and those with GPU or TPUs. In the resource list, one also sets the maximum time a job will be run and how much memory a job can take. Moreover, which is also relevant to proprietary software users, one specifies the necessary licences &#40;Mathematica, Matlab, etc.&#41;.</p>
<h2 id=running_julia ><a href="#running_julia" class=header-anchor >Running Julia</a></h2>
<p>If the previous script successfully produced a <code>test.log</code> file, we can take a small step to run Julia on the cluster. Instead of printing the nodes in the file <code>test.log</code>, we only need to pass them to a Julia.</p>
<p>As an example, let&#39;s consider a Julia program <code>code.jl</code>:</p>
<pre><code class="julia hljs"><span class=hljs-meta >@everywhere</span> f(x) = (println(x),x^<span class=hljs-number >2</span>)
res = pmap(f,<span class=hljs-number >1</span>:<span class=hljs-number >10</span>)

<span class=hljs-comment ># saving result</span>
<span class=hljs-comment ># ...</span></code></pre>
<p>which calculates the squares of numbers 1 to 10 and prints interactively what argument function receives. The result is saved to a file <code>code.log</code>.</p>
<p>To execute the code above on the cluster, we use a Julia machinefile option which makes execution as simple as</p>
<pre><code class="bash hljs">qsub --l:nodes=2:ppn=2 julia --machinefile=<span class=hljs-variable >$MachineFile</span> code.jl</code></pre>
<p>If that works <code>code.log</code> file is produced after the job has run. That is bare essentials to the usage of the cluster.</p>
<h2 id=interactive_mode ><a href="#interactive_mode" class=header-anchor >Interactive mode</a></h2>
<p>Sometimes things go wrong, and one needs to find out why. Issues could be numerous. The code has a bug for a cluster not configured correctly. One can use log files, <code>qstat</code> command to see the standard output or interactive mode, and sometimes a debug node to find these issues.</p>
<p>Interactive mode allows interaction with the shell environment where the job is executed. Thus, it is the most powerful of them all. To use it, one submits an ordinary job except that in place of the file we pass <code>-I</code> argument:</p>
<pre><code class="bash hljs">qsub --l:nodes=2:ppn=2 -I</code></pre>
<p>That would tell the scheduler to open the shell on the master process when the job starts.</p>
<p>In the opened shell then, we can execute the desired script of ours:</p>
<pre><code class="bash hljs">julia --machinefile=<span class=hljs-variable >$MachineFile</span> code.jl</code></pre>
<p>Or even initiate Julia process without file argument and then run it with <code>include&#40;&quot;code.jl&quot;&#41;</code>. It is particularly useful when a code has a bug and needs a quick fix for saving the output; one can also use it to see the progress of the calculation to see how long it would take to finish. Although unproductive, I admittedly am guilty of doing and enjoying that.</p>
<h2 id=array_jobs ><a href="#array_jobs" class=header-anchor >Array jobs</a></h2>
<p>Previously we used a master process delegated to us by the scheduler to initiate the cluster. That is only sometimes convenient. For example, one could wish to release allocated resources when work has finished on a particular worker. Also, the requirements for nodes and processors per node could become too bulky to schedule. Typically, scheduling multiple single-core jobs is more accessible, so some schedulers offer an array job option.</p>
<p>In contrast to an ordinary job, an array job collection of similar jobs is submitted on each core individually. To demonstrate that, we can write a script <code>test</code> which could print out the hostname of where the script had been executed &#40;again needs to be <code>chmod &#43;x test</code>&#41;.</p>
<pre><code class="bash hljs"><span class=hljs-meta >#!/bin/bash</span>
<span class=hljs-built_in >echo</span> hostname &gt;&gt; test.log</code></pre>
<p>To start an array job with a PBS scheduler, we give a <code>- t</code> flag to <code>qsub</code>. That tells the scheduler to execute the file on each allocated node or other words, literally to run <code>n</code> single-core jobs. To schedule the script above, we can do the following:</p>
<pre><code class="bash hljs">qsub -l:nodes=2:ppn=2 -t ./test</code></pre>
<p>In the resource list, we do not need to specify the nodes and processes per node &#40;although that might still work&#41;.</p>
<p>However, while this array job was relatively easy to schedule, it could have been more useful since we needed to coordinate what computation each core job required. For that, the PBS scheduler made it possible to find the id &#40;integer&#41; from the execution environment to tell the script which computations to execute. That is a somewhat archaic way to do these things and requires thinking &#40;which, as intelligent human beings, we want to avoid&#41; for setting up calculations.</p>
<h2 id=clustermanagersjl ><a href="#clustermanagersjl" class=header-anchor >ClusterManagers.jl</a></h2>
<p>Fortunately, we can use Julia to manage array jobs with the fabulous <code>ClusterManagers.jl</code> package. It gives us two options for initiating the cluster with array jobs. The first option is to start the TCP/IP server on the cluster login node and start array jobs which would start Julia processes and establish a connection with the master process by TCP/IP connection. The second option is to use a filesystem to connect the master and workers.</p>
<p>First, consider the TCP/IP connection between the master and workers. Before submitting an array job, we need to set up the listening process to which the workers connect. The second step is to submit an array job which initiates the workers, and then we need to wait until the scheduler puts it in the running state:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> Distributed
<span class=hljs-keyword >using</span> ClusterManagers
ElasticManager(cookie=<span class=hljs-string >&quot;foobar&quot;</span>)
<span class=hljs-comment ># Initiate workers</span>

run(<span class=hljs-string >`qsub -t &quot;echo \&quot;using ClusterManagers;
ClusterManagers.elastic_worker(\&quot;foobar\&quot;,\&quot;hpc05\&quot;,port=9000) &gt; julia&quot;`</span>)

<span class=hljs-keyword >while</span> nworkers()!=<span class=hljs-number >5</span>
    sleep(<span class=hljs-number >1</span>)
<span class=hljs-keyword >end</span>

pmap(x-&gt;x^<span class=hljs-number >2</span>,<span class=hljs-number >1</span>:<span class=hljs-number >10</span>)

<span class=hljs-comment ># To end the job </span>

kill(workers())</code></pre>
<p>The last step, after calculations, kills all workers and releases resources given by the scheduler.</p>
<p>Another possibility is to use the filesystem for communication with workers. For that ClusterManagers.jl offers a simple <code>addprocs_pbs</code>function which would schedule jobs and initiate workers with this simple function at once.</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> Distributed
<span class=hljs-keyword >using</span> ClusterManagers
addprocs_pbs(<span class=hljs-number >5</span>)
pmap(x-&gt;x^<span class=hljs-number >2</span>,<span class=hljs-number >1</span>:<span class=hljs-number >10</span>)</code></pre>
<p>Ultimately, one can exit the Julia process or remove all allocated workers to finish the job.</p>
<p>I extensively used communication through the filesystem for the last project because that was the only thing I knew. I found it annoyingly slow to initiate, so I started to think about better ways of initiating it, and that&#39;s how I eventually got these experiences on how to initiate the Cluster.</p>
<h2 id=conclusion ><a href="#conclusion" class=header-anchor >Conclusion</a></h2>
<p>After reflecting on this blog post, Julia has excelled in giving excellent tools for parallel computing. Moreover, with this post, I have shown that it is relatively easy to start using the cluster. Nevertheless, the tools could be more comfortable.</p>
<p>Particularly the ClusterManagers.jl could make it equally easy to initiate the PBS cluster with TCP/IP connection as it is with the filesystem. Also, the scheduler does not always support array jobs; thus, it would be nice to optionally use the ordinary scheduling &#40;giving nodes and processes per node&#41;.</p>
<p>Moreover, we could significantly improve how we interact with the cluster. We are accustomed to ssh into the cluster, sending our code there, scheduling the job and transferring the data. A better way, I envision, would be to create a macro which, from the user perspective, could work similarly to <code>@spawn</code> and return a remote reference something like:</p>
<pre><code class="julia hljs">N = <span class=hljs-number >100</span> <span class=hljs-comment ># The @cluster macro would see that</span>

<span class=hljs-meta >@cluster</span> hpc05 <span class=hljs-number >10</span> <span class=hljs-keyword >begin</span> <span class=hljs-comment ># .juliarc could host cluster information</span>
    <span class=hljs-keyword >using</span> Distributed
    <span class=hljs-keyword >using</span> MyModule <span class=hljs-comment ># Modules would be synchronised with the cluster automatically, as also the necessary source files.</span>
    res = pmap(x-&gt;(println(x),f(x)),N) <span class=hljs-comment ># Text output would be redirected locally</span>
<span class=hljs-keyword >end</span>

res <span class=hljs-comment ># All variables in the global scope of the cluster would be visible locally afterwards execution and possibly also during that.</span></code></pre>
<div class=page-foot >
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Janis Erdmanis. Last modified: August 11, 2023.
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
</div>
</div>
    
    
        <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>